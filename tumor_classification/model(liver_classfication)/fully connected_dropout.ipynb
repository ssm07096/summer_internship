{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchsummary\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일이름 sort해서 list저장\n",
    "data_path='/disk1/data_liverbound_noclip/'\n",
    "name_list=os.listdir(data_path)\n",
    "\n",
    "segmentation_data = [files[:-4] for files in name_list if files.startswith('segmentation')]\n",
    "segmentation_data=list(set(segmentation_data))\n",
    "segmentation_data.sort()\n",
    "seg_data_test=[]\n",
    "for i in range(30,45):\n",
    "    seg_data_test.append(segmentation_data.pop(i))\n",
    "\n",
    "volume_data=[files[:-4] for files in name_list if files.startswith('volume')]\n",
    "volume_data=list(set(volume_data))\n",
    "volume_data.sort()\n",
    "vol_data_test=[]\n",
    "for i in range(30,45):\n",
    "    vol_data_test.append(volume_data.pop(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_labels=np.loadtxt('/home/sumins/workspace/liver_classification/all_labels.txt',dtype=int)\n",
    "# all_labels=all_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation-0\n",
      "segmentation-1\n",
      "segmentation-10\n",
      "segmentation-100\n",
      "segmentation-101\n",
      "segmentation-102\n",
      "segmentation-103\n",
      "segmentation-104\n",
      "segmentation-105\n",
      "segmentation-106\n",
      "segmentation-107\n",
      "segmentation-108\n",
      "segmentation-109\n",
      "segmentation-11\n",
      "segmentation-110\n",
      "segmentation-111\n",
      "segmentation-112\n",
      "segmentation-113\n",
      "segmentation-114\n",
      "segmentation-115\n",
      "segmentation-116\n",
      "segmentation-117\n",
      "segmentation-118\n",
      "segmentation-119\n",
      "segmentation-12\n",
      "segmentation-120\n",
      "segmentation-121\n",
      "segmentation-122\n",
      "segmentation-123\n",
      "segmentation-124\n",
      "segmentation-126\n",
      "segmentation-128\n",
      "segmentation-13\n",
      "segmentation-15\n",
      "segmentation-17\n",
      "segmentation-19\n",
      "segmentation-20\n",
      "segmentation-22\n",
      "segmentation-24\n",
      "segmentation-26\n",
      "segmentation-28\n",
      "segmentation-3\n",
      "segmentation-31\n",
      "segmentation-33\n",
      "segmentation-35\n",
      "segmentation-36\n",
      "segmentation-37\n",
      "segmentation-38\n",
      "segmentation-39\n",
      "segmentation-4\n",
      "segmentation-40\n",
      "segmentation-41\n",
      "segmentation-42\n",
      "segmentation-43\n",
      "segmentation-44\n",
      "segmentation-45\n",
      "segmentation-46\n",
      "segmentation-47\n",
      "segmentation-48\n",
      "segmentation-49\n",
      "segmentation-5\n",
      "segmentation-50\n",
      "segmentation-51\n",
      "segmentation-52\n",
      "segmentation-53\n",
      "segmentation-54\n",
      "segmentation-55\n",
      "segmentation-56\n",
      "segmentation-57\n",
      "segmentation-58\n",
      "segmentation-59\n",
      "segmentation-6\n",
      "segmentation-60\n",
      "segmentation-61\n",
      "segmentation-62\n",
      "segmentation-63\n",
      "segmentation-64\n",
      "segmentation-65\n",
      "segmentation-66\n",
      "segmentation-67\n",
      "segmentation-68\n",
      "segmentation-69\n",
      "segmentation-7\n",
      "segmentation-70\n",
      "segmentation-71\n",
      "segmentation-72\n",
      "segmentation-73\n",
      "segmentation-74\n",
      "segmentation-75\n",
      "segmentation-76\n",
      "segmentation-77\n",
      "segmentation-78\n",
      "segmentation-79\n",
      "segmentation-8\n",
      "segmentation-80\n",
      "segmentation-81\n",
      "segmentation-82\n",
      "segmentation-83\n",
      "segmentation-84\n",
      "segmentation-85\n",
      "segmentation-86\n",
      "segmentation-87\n",
      "segmentation-88\n",
      "segmentation-89\n",
      "segmentation-9\n",
      "segmentation-90\n",
      "segmentation-91\n",
      "segmentation-92\n",
      "segmentation-93\n",
      "segmentation-94\n",
      "segmentation-95\n",
      "segmentation-96\n",
      "segmentation-97\n",
      "segmentation-98\n",
      "segmentation-99\n",
      "segmentation-125\n",
      "133\n",
      "segmentation-127\n",
      "270\n",
      "segmentation-129\n",
      "325\n",
      "segmentation-14\n",
      "164\n",
      "segmentation-16\n",
      "222\n",
      "segmentation-18\n",
      "224\n",
      "segmentation-2\n",
      "164\n",
      "segmentation-21\n",
      "191\n",
      "segmentation-23\n",
      "137\n",
      "segmentation-25\n",
      "277\n",
      "segmentation-27\n",
      "272\n",
      "segmentation-29\n",
      "135\n",
      "segmentation-30\n",
      "146\n",
      "segmentation-32\n",
      "128\n",
      "segmentation-34\n",
      "109\n",
      "volume-0\n",
      "volume-1\n",
      "volume-10\n",
      "volume-100\n",
      "volume-101\n",
      "volume-102\n",
      "volume-103\n",
      "volume-104\n",
      "volume-105\n",
      "volume-106\n",
      "volume-107\n",
      "volume-108\n",
      "volume-109\n",
      "volume-11\n",
      "volume-110\n",
      "volume-111\n",
      "volume-112\n",
      "volume-113\n",
      "volume-114\n",
      "volume-115\n",
      "volume-116\n",
      "volume-117\n",
      "volume-118\n",
      "volume-119\n",
      "volume-12\n",
      "volume-120\n",
      "volume-121\n",
      "volume-122\n",
      "volume-123\n",
      "volume-124\n",
      "volume-126\n",
      "volume-128\n",
      "volume-13\n",
      "volume-15\n",
      "volume-17\n",
      "volume-19\n",
      "volume-20\n",
      "volume-22\n",
      "volume-24\n",
      "volume-26\n",
      "volume-28\n",
      "volume-3\n",
      "volume-31\n",
      "volume-33\n",
      "volume-35\n",
      "volume-36\n",
      "volume-37\n",
      "volume-38\n",
      "volume-39\n",
      "volume-4\n",
      "volume-40\n",
      "volume-41\n",
      "volume-42\n",
      "volume-43\n",
      "volume-44\n",
      "volume-45\n",
      "volume-46\n",
      "volume-47\n",
      "volume-48\n",
      "volume-49\n",
      "volume-5\n",
      "volume-50\n",
      "volume-51\n",
      "volume-52\n",
      "volume-53\n",
      "volume-54\n",
      "volume-55\n",
      "volume-56\n",
      "volume-57\n",
      "volume-58\n",
      "volume-59\n",
      "volume-6\n",
      "volume-60\n",
      "volume-61\n",
      "volume-62\n",
      "volume-63\n",
      "volume-64\n",
      "volume-65\n",
      "volume-66\n",
      "volume-67\n",
      "volume-68\n",
      "volume-69\n",
      "volume-7\n",
      "volume-70\n",
      "volume-71\n",
      "volume-72\n",
      "volume-73\n",
      "volume-74\n",
      "volume-75\n",
      "volume-76\n",
      "volume-77\n",
      "volume-78\n",
      "volume-79\n",
      "volume-8\n",
      "volume-80\n",
      "volume-81\n",
      "volume-82\n",
      "volume-83\n",
      "volume-84\n",
      "volume-85\n",
      "volume-86\n",
      "volume-87\n",
      "volume-88\n",
      "volume-89\n",
      "volume-9\n",
      "volume-90\n",
      "volume-91\n",
      "volume-92\n",
      "volume-93\n",
      "volume-94\n",
      "volume-95\n",
      "volume-96\n",
      "volume-97\n",
      "volume-98\n",
      "volume-99\n",
      "volume-125\n",
      "volume-127\n",
      "volume-129\n",
      "volume-14\n",
      "volume-16\n",
      "volume-18\n",
      "volume-2\n",
      "volume-21\n",
      "volume-23\n",
      "volume-25\n",
      "volume-27\n",
      "volume-29\n",
      "volume-30\n",
      "volume-32\n",
      "volume-34\n"
     ]
    }
   ],
   "source": [
    "#npy를 slice별로 나누어 하나의 list저장\n",
    "seg_list_train=[]\n",
    "seg_list_test=[]\n",
    "for file in segmentation_data:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        #print(total_slices)\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            seg_list_train.append(img_array[:,:,current_slice]) \n",
    "\n",
    "for file in seg_data_test:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        print(total_slices)\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            seg_list_test.append(img_array[:,:,current_slice]) \n",
    "#간 1, 병변 2, 나머지 0\n",
    "\n",
    "\n",
    "vol_list_train=[]\n",
    "vol_list_test=[]\n",
    "for file in volume_data:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            vol_list_train.append(img_array[:,:,current_slice]) \n",
    "            \n",
    "for file in vol_data_test:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            vol_list_test.append(img_array[:,:,current_slice]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label을 만들어 list에 저장\n",
    "labels_train = []\n",
    "labels_test=[]\n",
    "for i in seg_list_test:\n",
    "    if 2 in i:\n",
    "        labels_test.append(1)\n",
    "    else:\n",
    "        labels_test.append(0)\n",
    "        \n",
    "for i in seg_list_train:\n",
    "    if 2 in i:\n",
    "        labels_train.append(1)\n",
    "    else:\n",
    "        labels_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0=0\n",
    "train_1=0\n",
    "test_0=0\n",
    "test_1=0\n",
    "for i in labels_train:\n",
    "    if i==0:\n",
    "        train_0+=1\n",
    "    elif i==1:\n",
    "        train_1+=1\n",
    "for i in labels_test:\n",
    "    if i==0:\n",
    "        test_0+=1\n",
    "    elif i==1:\n",
    "        test_1+=1\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13062\n",
      "6248\n",
      "2155\n",
      "742\n"
     ]
    }
   ],
   "source": [
    "print(train_0)\n",
    "print(train_1)\n",
    "print(test_0)\n",
    "print(test_1)\n",
    "\n",
    "#15000 7000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(nparray):\n",
    "    # normalize scans to [0,1]\n",
    "    _min = nparray.min()\n",
    "    _max = nparray.max()\n",
    "    nparray = nparray - _min\n",
    "    nparray = nparray / (_max - _min)\n",
    "    return nparray\n",
    "\n",
    "def norm_zscore(nparray):\n",
    "    # normalize 2d scands by mean and standard deviation\n",
    "    mean = nparray.mean()\n",
    "    std = nparray.std()    \n",
    "    nparray = nparray - mean\n",
    "    nparray /= std\n",
    "    return nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_MAX = 150\n",
    "WINDOW_MIN = -50\n",
    "GLOBAL_PIXEL_MEAN = 0.1\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "  def __init__(self,volume_list,all_labels,transforms=None):\n",
    "    self.volume_list=volume_list\n",
    "    self.all_labels=all_labels\n",
    "    self.length=len(all_labels)\n",
    "    self.transforms=transforms\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    npy=self.volume_list[idx]\n",
    "\n",
    "    npy[npy > WINDOW_MAX] = WINDOW_MAX\n",
    "    npy[npy < WINDOW_MIN] = WINDOW_MIN\n",
    "    \n",
    "    npy = (npy - WINDOW_MIN) / (WINDOW_MAX - WINDOW_MIN)\n",
    "    npy -= GLOBAL_PIXEL_MEAN\n",
    "    \n",
    "    if len(npy.shape)==2:\n",
    "      npy=npy[:,:,np.newaxis].astype(dtype='float32')\n",
    "    \n",
    "    if self.transforms is not None:\n",
    "      npy=self.transforms(npy)\n",
    "    \n",
    "    return{'npy':npy,'label':self.all_labels[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 32, 512, 512]             832\n",
      "       BatchNorm2d-2         [16, 32, 512, 512]              64\n",
      "         MaxPool2d-3         [16, 32, 256, 256]               0\n",
      "           Dropout-4         [16, 32, 256, 256]               0\n",
      "            Conv2d-5         [16, 64, 256, 256]          51,264\n",
      "       BatchNorm2d-6         [16, 64, 256, 256]             128\n",
      "         MaxPool2d-7         [16, 64, 128, 128]               0\n",
      "           Dropout-8         [16, 64, 128, 128]               0\n",
      "            Conv2d-9        [16, 128, 128, 128]         204,928\n",
      "      BatchNorm2d-10        [16, 128, 128, 128]             256\n",
      "          Dropout-11        [16, 128, 128, 128]               0\n",
      "           Conv2d-12        [16, 128, 128, 128]         409,728\n",
      "        MaxPool2d-13          [16, 128, 64, 64]               0\n",
      "          Dropout-14          [16, 128, 64, 64]               0\n",
      "           Linear-15                   [16, 64]      33,554,496\n",
      "      BatchNorm1d-16                   [16, 64]             128\n",
      "          Dropout-17                   [16, 64]               0\n",
      "           Linear-18                    [16, 2]             130\n",
      "================================================================\n",
      "Total params: 34,221,954\n",
      "Trainable params: 34,221,954\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 16.00\n",
      "Forward/backward pass size (MB): 4992.02\n",
      "Params size (MB): 130.55\n",
      "Estimated Total Size (MB): 5138.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.batch_32 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.batch_64 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.batch_128 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5, padding=2)\n",
    "\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128*64*64, out_features=64)\n",
    "        self.batch_fc = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.out = nn.Linear(in_features=64, out_features=2)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_32(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_64(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_128(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3_2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc1(x.view(-1,128*64*64))\n",
    "        x = self.batch_fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model=Net().cuda()\n",
    "torchsummary.summary(model, (1,512,512),batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([transforms.ToTensor()\n",
    "                                       ])\n",
    "transforms_test = transforms.Compose([transforms.ToTensor()\n",
    "                                       ])\n",
    "\n",
    "hyper_param_epoch=50\n",
    "hyper_param_batch=16\n",
    "hyper_param_learning_rate=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vol_train, vol_valid, lab_train, lab_valid = train_test_split(volume_list, all_labels, test_size=0.3, shuffle=True, stratify=all_labels, random_state=34)\n",
    "train_dataset=CustomDataset(volume_list=vol_list_train, all_labels=labels_train,transforms=transforms_train)\n",
    "test_dataset=CustomDataset(volume_list=vol_list_test,all_labels=labels_test,transforms=transforms_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyper_param_batch, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyper_param_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 / 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device,\"/\" ,torch.cuda.device_count())\n",
    "\n",
    "custom_model=Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(custom_model.parameters(), lr=hyper_param_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(model):\n",
    "    total_loss=0\n",
    "    for i_batch, item in enumerate(test_loader):\n",
    "        npys = item['npy'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "        if(len(labels)!=hyper_param_batch):\n",
    "            i_batch-=1\n",
    "            break\n",
    "        outputs =model(npys)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "    return total_loss/(i_batch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50],i_batch=1207 ,Train_Loss: 0.2936,Valid_loss: 0.6924\n",
      "Time: 304.703284740448sec\n",
      "Epoch [2/50],i_batch=1207 ,Train_Loss: 0.0727,Valid_loss: 0.9621\n",
      "Time: 289.97355484962463sec\n",
      "Epoch [3/50],i_batch=1207 ,Train_Loss: 0.0665,Valid_loss: 0.9810\n",
      "Time: 396.30564308166504sec\n",
      "Epoch [4/50],i_batch=1207 ,Train_Loss: 0.1225,Valid_loss: 1.1561\n",
      "Time: 1190.2253723144531sec\n",
      "Epoch [5/50],i_batch=1207 ,Train_Loss: 0.0320,Valid_loss: 1.1759\n",
      "Time: 388.3869924545288sec\n",
      "Epoch [6/50],i_batch=1207 ,Train_Loss: 0.0289,Valid_loss: 1.3044\n",
      "Time: 1322.0510506629944sec\n",
      "Epoch [7/50],i_batch=1207 ,Train_Loss: 0.1807,Valid_loss: 1.3441\n",
      "Time: 962.7236394882202sec\n",
      "Epoch [8/50],i_batch=1207 ,Train_Loss: 0.0327,Valid_loss: 1.3317\n",
      "Time: 618.9200487136841sec\n",
      "Epoch [9/50],i_batch=1207 ,Train_Loss: 0.0775,Valid_loss: 1.4514\n",
      "Time: 650.5282728672028sec\n",
      "Epoch [10/50],i_batch=1207 ,Train_Loss: 0.0130,Valid_loss: 1.4283\n",
      "Time: 1303.6850399971008sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e9463f745700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_param_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mnpys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'npy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b25f90b2ed4b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mnpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'npy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnpy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;31m# backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_value=1\n",
    "torch.cuda.empty_cache()\n",
    "start=time.time()\n",
    "custom_model.train()\n",
    "train_loss_history=[]\n",
    "valid_loss_history=[]\n",
    "val_loss=0\n",
    "for e in range(hyper_param_epoch):\n",
    "        for i_batch, item in enumerate(train_loader):\n",
    "                npys = item['npy'].to(device)\n",
    "                labels = item['label'].to(device)\n",
    "                if(len(labels)!=hyper_param_batch):\n",
    "                        break\n",
    "                #print(npys)\n",
    "                # Forward pass\n",
    "                outputs =custom_model(npys)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        val_loss=validation_loss(custom_model)\n",
    "        train_loss_history.append(loss.item())\n",
    "        valid_loss_history.append(val_loss)\n",
    "        print('Epoch [{}/{}],i_batch={} ,Train_Loss: {:.4f},Valid_loss: {:.4f}'\n",
    "                                        .format(e + 1, hyper_param_epoch, i_batch+1, loss.item(),val_loss))\n",
    "        print(\"Time: {}sec\".format(time.time()-start))\n",
    "        start=time.time()\n",
    "        if loss_value>val_loss:\n",
    "                loss_value=val_loss\n",
    "                torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': custom_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, '/home/sumins/workspace/model_check/model_0722_2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.plot(valid_loss_history)\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open(\"Valid_dropout.csv\", 'w') as file:\n",
    "  writer = csv.writer(file)\n",
    "  writer.writerow(valid_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open(\"Train_dropout.csv\", 'w') as file:\n",
    "  writer = csv.writer(file)\n",
    "  writer.writerow(train_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2936, device='cuda:0', requires_grad=True)\n",
      "Test Accuracy of the model on the 2896 test images: 65.12430939226519 %\n"
     ]
    }
   ],
   "source": [
    "true_label=[]\n",
    "pred_label=[]\n",
    "\n",
    "model=Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_param_learning_rate)\n",
    "\n",
    "checkpoint = torch.load('/home/sumins/workspace/model_check/model_0722_2.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(loss)\n",
    "custom_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for item in test_loader:\n",
    "        npys = item['npy'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "        if(len(labels)!=hyper_param_batch):\n",
    "            break\n",
    "        outputs =model(npys)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        true_label.extend(labels)\n",
    "        pred_label.extend(predicted)\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the {} test images: {} %'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.76      2155\n",
      "           1       0.35      0.42      0.38       741\n",
      "\n",
      "    accuracy                           0.65      2896\n",
      "   macro avg       0.57      0.57      0.57      2896\n",
      "weighted avg       0.67      0.65      0.66      2896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels=torch.tensor(true_label)\n",
    "true_labels=true_labels.tolist()\n",
    "pred_labels=torch.tensor(pred_label)\n",
    "pred_labels=pred_labels.tolist()\n",
    "print(classification_report(true_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0803, device='cuda:0', requires_grad=True)\n",
      "Test Accuracy of the model on the 19310 train images: 99.98964267219057 %\n"
     ]
    }
   ],
   "source": [
    "true_label1=[]\n",
    "pred_label1=[]\n",
    "\n",
    "model=CustomConvNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_param_learning_rate)\n",
    "\n",
    "checkpoint = torch.load('/home/sumins/workspace/model_check/model_0721.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(loss)\n",
    "custom_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for item in train_loader:\n",
    "        npys = item['npy'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "        \n",
    "        outputs =model(npys)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        true_label1.extend(labels)\n",
    "        pred_label1.extend(predicted)\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the {} train images: {} %'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     13062\n",
      "           1       1.00      1.00      1.00      6248\n",
      "\n",
      "    accuracy                           1.00     19310\n",
      "   macro avg       1.00      1.00      1.00     19310\n",
      "weighted avg       1.00      1.00      1.00     19310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels1=torch.tensor(true_label1)\n",
    "true_labels1=true_labels1.tolist()\n",
    "pred_labels1=torch.tensor(pred_label1)\n",
    "pred_labels1=pred_labels1.tolist()\n",
    "print(classification_report(true_labels1,pred_labels1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
