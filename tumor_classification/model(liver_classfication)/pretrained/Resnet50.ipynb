{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchsummary\n",
    "from torchvision import transforms, models\n",
    "import time\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일이름 sort해서 list저장\n",
    "data_path='/disk1/data_liverbound_noclip/'\n",
    "name_list=os.listdir(data_path)\n",
    "\n",
    "segmentation_data = [files[:-4] for files in name_list if files.startswith('segmentation')]\n",
    "segmentation_data=list(set(segmentation_data))\n",
    "segmentation_data.sort()\n",
    "seg_data_test=[]\n",
    "for i in range(30,45):\n",
    "    seg_data_test.append(segmentation_data.pop(i))\n",
    "\n",
    "volume_data=[files[:-4] for files in name_list if files.startswith('volume')]\n",
    "volume_data=list(set(volume_data))\n",
    "volume_data.sort()\n",
    "vol_data_test=[]\n",
    "for i in range(30,45):\n",
    "    vol_data_test.append(volume_data.pop(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_labels=np.loadtxt('/home/sumins/workspace/liver_classification/all_labels.txt',dtype=int)\n",
    "# all_labels=all_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation-0\n",
      "segmentation-1\n",
      "segmentation-10\n",
      "segmentation-100\n",
      "segmentation-101\n",
      "segmentation-102\n",
      "segmentation-103\n",
      "segmentation-104\n",
      "segmentation-105\n",
      "segmentation-106\n",
      "segmentation-107\n",
      "segmentation-108\n",
      "segmentation-109\n",
      "segmentation-11\n",
      "segmentation-110\n",
      "segmentation-111\n",
      "segmentation-112\n",
      "segmentation-113\n",
      "segmentation-114\n",
      "segmentation-115\n",
      "segmentation-116\n",
      "segmentation-117\n",
      "segmentation-118\n",
      "segmentation-119\n",
      "segmentation-12\n",
      "segmentation-120\n",
      "segmentation-121\n",
      "segmentation-122\n",
      "segmentation-123\n",
      "segmentation-124\n",
      "segmentation-126\n",
      "segmentation-128\n",
      "segmentation-13\n",
      "segmentation-15\n",
      "segmentation-17\n",
      "segmentation-19\n",
      "segmentation-20\n",
      "segmentation-22\n",
      "segmentation-24\n",
      "segmentation-26\n",
      "segmentation-28\n",
      "segmentation-3\n",
      "segmentation-31\n",
      "segmentation-33\n",
      "segmentation-35\n",
      "segmentation-36\n",
      "segmentation-37\n",
      "segmentation-38\n",
      "segmentation-39\n",
      "segmentation-4\n",
      "segmentation-40\n",
      "segmentation-41\n",
      "segmentation-42\n",
      "segmentation-43\n",
      "segmentation-44\n",
      "segmentation-45\n",
      "segmentation-46\n",
      "segmentation-47\n",
      "segmentation-48\n",
      "segmentation-49\n",
      "segmentation-5\n",
      "segmentation-50\n",
      "segmentation-51\n",
      "segmentation-52\n",
      "segmentation-53\n",
      "segmentation-54\n",
      "segmentation-55\n",
      "segmentation-56\n",
      "segmentation-57\n",
      "segmentation-58\n",
      "segmentation-59\n",
      "segmentation-6\n",
      "segmentation-60\n",
      "segmentation-61\n",
      "segmentation-62\n",
      "segmentation-63\n",
      "segmentation-64\n",
      "segmentation-65\n",
      "segmentation-66\n",
      "segmentation-67\n",
      "segmentation-68\n",
      "segmentation-69\n",
      "segmentation-7\n",
      "segmentation-70\n",
      "segmentation-71\n",
      "segmentation-72\n",
      "segmentation-73\n",
      "segmentation-74\n",
      "segmentation-75\n",
      "segmentation-76\n",
      "segmentation-77\n",
      "segmentation-78\n",
      "segmentation-79\n",
      "segmentation-8\n",
      "segmentation-80\n",
      "segmentation-81\n",
      "segmentation-82\n",
      "segmentation-83\n",
      "segmentation-84\n",
      "segmentation-85\n",
      "segmentation-86\n",
      "segmentation-87\n",
      "segmentation-88\n",
      "segmentation-89\n",
      "segmentation-9\n",
      "segmentation-90\n",
      "segmentation-91\n",
      "segmentation-92\n",
      "segmentation-93\n",
      "segmentation-94\n",
      "segmentation-95\n",
      "segmentation-96\n",
      "segmentation-97\n",
      "segmentation-98\n",
      "segmentation-99\n",
      "segmentation-125\n",
      "133\n",
      "segmentation-127\n",
      "270\n",
      "segmentation-129\n",
      "325\n",
      "segmentation-14\n",
      "164\n",
      "segmentation-16\n",
      "222\n",
      "segmentation-18\n",
      "224\n",
      "segmentation-2\n",
      "164\n",
      "segmentation-21\n",
      "191\n",
      "segmentation-23\n",
      "137\n",
      "segmentation-25\n",
      "277\n",
      "segmentation-27\n",
      "272\n",
      "segmentation-29\n",
      "135\n",
      "segmentation-30\n",
      "146\n",
      "segmentation-32\n",
      "128\n",
      "segmentation-34\n",
      "109\n",
      "volume-0\n",
      "volume-1\n",
      "volume-10\n",
      "volume-100\n",
      "volume-101\n",
      "volume-102\n",
      "volume-103\n",
      "volume-104\n",
      "volume-105\n",
      "volume-106\n",
      "volume-107\n",
      "volume-108\n",
      "volume-109\n",
      "volume-11\n",
      "volume-110\n",
      "volume-111\n",
      "volume-112\n",
      "volume-113\n",
      "volume-114\n",
      "volume-115\n",
      "volume-116\n",
      "volume-117\n",
      "volume-118\n",
      "volume-119\n",
      "volume-12\n",
      "volume-120\n",
      "volume-121\n",
      "volume-122\n",
      "volume-123\n",
      "volume-124\n",
      "volume-126\n",
      "volume-128\n",
      "volume-13\n",
      "volume-15\n",
      "volume-17\n",
      "volume-19\n",
      "volume-20\n",
      "volume-22\n",
      "volume-24\n",
      "volume-26\n",
      "volume-28\n",
      "volume-3\n",
      "volume-31\n",
      "volume-33\n",
      "volume-35\n",
      "volume-36\n",
      "volume-37\n",
      "volume-38\n",
      "volume-39\n",
      "volume-4\n",
      "volume-40\n",
      "volume-41\n",
      "volume-42\n",
      "volume-43\n",
      "volume-44\n",
      "volume-45\n",
      "volume-46\n",
      "volume-47\n",
      "volume-48\n",
      "volume-49\n",
      "volume-5\n",
      "volume-50\n",
      "volume-51\n",
      "volume-52\n",
      "volume-53\n",
      "volume-54\n",
      "volume-55\n",
      "volume-56\n",
      "volume-57\n",
      "volume-58\n",
      "volume-59\n",
      "volume-6\n",
      "volume-60\n",
      "volume-61\n",
      "volume-62\n",
      "volume-63\n",
      "volume-64\n",
      "volume-65\n",
      "volume-66\n",
      "volume-67\n",
      "volume-68\n",
      "volume-69\n",
      "volume-7\n",
      "volume-70\n",
      "volume-71\n",
      "volume-72\n",
      "volume-73\n",
      "volume-74\n",
      "volume-75\n",
      "volume-76\n",
      "volume-77\n",
      "volume-78\n",
      "volume-79\n",
      "volume-8\n",
      "volume-80\n",
      "volume-81\n",
      "volume-82\n",
      "volume-83\n",
      "volume-84\n",
      "volume-85\n",
      "volume-86\n",
      "volume-87\n",
      "volume-88\n",
      "volume-89\n",
      "volume-9\n",
      "volume-90\n",
      "volume-91\n",
      "volume-92\n",
      "volume-93\n",
      "volume-94\n",
      "volume-95\n",
      "volume-96\n",
      "volume-97\n",
      "volume-98\n",
      "volume-99\n",
      "volume-125\n",
      "volume-127\n",
      "volume-129\n",
      "volume-14\n",
      "volume-16\n",
      "volume-18\n",
      "volume-2\n",
      "volume-21\n",
      "volume-23\n",
      "volume-25\n",
      "volume-27\n",
      "volume-29\n",
      "volume-30\n",
      "volume-32\n",
      "volume-34\n"
     ]
    }
   ],
   "source": [
    "#npy를 slice별로 나누어 하나의 list저장\n",
    "seg_list_train=[]\n",
    "seg_list_test=[]\n",
    "for file in segmentation_data:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        #print(total_slices)\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            seg_list_train.append(img_array[:,:,current_slice]) \n",
    "\n",
    "for file in seg_data_test:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        print(total_slices)\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            seg_list_test.append(img_array[:,:,current_slice]) \n",
    "#간 1, 병변 2, 나머지 0\n",
    "\n",
    "\n",
    "vol_list_train=[]\n",
    "vol_list_test=[]\n",
    "for file in volume_data:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            vol_list_train.append(img_array[:,:,current_slice]) \n",
    "            \n",
    "for file in vol_data_test:\n",
    "    fname=os.path.basename(file)\n",
    "    print(fname)\n",
    "    img_array=np.load(data_path+fname+'.npy')\n",
    "    \n",
    "    #print(f'img_num: {img_array.shape}')\n",
    "    \n",
    "    if len(img_array.shape) == 3:\n",
    "        nx, ny, nz = img_array.shape\n",
    "        total_slices = img_array.shape[2]\n",
    "        # iterate through slices\n",
    "        for current_slice in range(0, total_slices):\n",
    "            vol_list_test.append(img_array[:,:,current_slice]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label을 만들어 list에 저장\n",
    "labels_train = []\n",
    "labels_test=[]\n",
    "for i in seg_list_test:\n",
    "    if 2 in i:\n",
    "        labels_test.append(1)\n",
    "    else:\n",
    "        labels_test.append(0)\n",
    "        \n",
    "for i in seg_list_train:\n",
    "    if 2 in i:\n",
    "        labels_train.append(1)\n",
    "    else:\n",
    "        labels_train.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(nparray):\n",
    "    # normalize scans to [0,1]\n",
    "    _min = nparray.min()\n",
    "    _max = nparray.max()\n",
    "    nparray = nparray - _min\n",
    "    nparray = nparray / (_max - _min)\n",
    "    return nparray\n",
    "\n",
    "def norm_zscore(nparray):\n",
    "    # normalize 2d scands by mean and standard deviation\n",
    "    mean = nparray.mean()\n",
    "    std = nparray.std()    \n",
    "    nparray = nparray - mean\n",
    "    nparray /= std\n",
    "    return nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_MAX = 200\n",
    "WINDOW_MIN = 0\n",
    "GLOBAL_PIXEL_MEAN = 0.1\n",
    "\n",
    "class CustomDataset(Dataset): \n",
    "  def __init__(self,volume_list,all_labels,transforms=None):\n",
    "    self.volume_list=volume_list\n",
    "    self.all_labels=all_labels\n",
    "    self.length=len(all_labels)\n",
    "    self.transforms=transforms\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.length\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    npy=self.volume_list[idx]\n",
    "\n",
    "    npy[npy > WINDOW_MAX] = WINDOW_MAX\n",
    "    npy[npy < WINDOW_MIN] = WINDOW_MIN\n",
    "    \n",
    "    npy = (npy - WINDOW_MIN) / (WINDOW_MAX - WINDOW_MIN)\n",
    "    npy -= GLOBAL_PIXEL_MEAN\n",
    "    \n",
    "    if len(npy.shape)==2:\n",
    "      npy=npy[:,:,np.newaxis].astype(dtype='float32')\n",
    "    \n",
    "    if self.transforms is not None:\n",
    "      npy=self.transforms(npy)\n",
    "    \n",
    "    return{'npy':npy,'label':self.all_labels[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "resnet50_pretrained=models.resnet50(pretrained=True)\n",
    "print(resnet50_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiverResnet(nn.Module):\n",
    "    def __init__(self,in_channels=1):\n",
    "        super(LiverResnet,self).__init__()\n",
    "        \n",
    "        #torchvision.models에서 사전훈련된 resnet모델 가져오기\n",
    "        self.model=models.resnet50(pretrained=True)\n",
    "        \n",
    "        #기본채널이 3이기 때문에 liver data set에 맞게 1로 바꿔줌\n",
    "        #원래 resnet의 첫번째 layer\n",
    "        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.model.conv1=nn.Conv2d(in_channels,64,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "        \n",
    "        #class수 변경\n",
    "        num_ftrs=self.model.fc.in_features\n",
    "        self.model.fc=nn.Linear(num_ftrs,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "            return self.model(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 256, 256]           3,136\n",
      "       BatchNorm2d-2          [1, 64, 256, 256]             128\n",
      "              ReLU-3          [1, 64, 256, 256]               0\n",
      "         MaxPool2d-4          [1, 64, 128, 128]               0\n",
      "            Conv2d-5          [1, 64, 128, 128]           4,096\n",
      "       BatchNorm2d-6          [1, 64, 128, 128]             128\n",
      "              ReLU-7          [1, 64, 128, 128]               0\n",
      "            Conv2d-8          [1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-9          [1, 64, 128, 128]             128\n",
      "             ReLU-10          [1, 64, 128, 128]               0\n",
      "           Conv2d-11         [1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-12         [1, 256, 128, 128]             512\n",
      "           Conv2d-13         [1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-14         [1, 256, 128, 128]             512\n",
      "             ReLU-15         [1, 256, 128, 128]               0\n",
      "       Bottleneck-16         [1, 256, 128, 128]               0\n",
      "           Conv2d-17          [1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-18          [1, 64, 128, 128]             128\n",
      "             ReLU-19          [1, 64, 128, 128]               0\n",
      "           Conv2d-20          [1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-21          [1, 64, 128, 128]             128\n",
      "             ReLU-22          [1, 64, 128, 128]               0\n",
      "           Conv2d-23         [1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-24         [1, 256, 128, 128]             512\n",
      "             ReLU-25         [1, 256, 128, 128]               0\n",
      "       Bottleneck-26         [1, 256, 128, 128]               0\n",
      "           Conv2d-27          [1, 64, 128, 128]          16,384\n",
      "      BatchNorm2d-28          [1, 64, 128, 128]             128\n",
      "             ReLU-29          [1, 64, 128, 128]               0\n",
      "           Conv2d-30          [1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-31          [1, 64, 128, 128]             128\n",
      "             ReLU-32          [1, 64, 128, 128]               0\n",
      "           Conv2d-33         [1, 256, 128, 128]          16,384\n",
      "      BatchNorm2d-34         [1, 256, 128, 128]             512\n",
      "             ReLU-35         [1, 256, 128, 128]               0\n",
      "       Bottleneck-36         [1, 256, 128, 128]               0\n",
      "           Conv2d-37         [1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-38         [1, 128, 128, 128]             256\n",
      "             ReLU-39         [1, 128, 128, 128]               0\n",
      "           Conv2d-40           [1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-41           [1, 128, 64, 64]             256\n",
      "             ReLU-42           [1, 128, 64, 64]               0\n",
      "           Conv2d-43           [1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-44           [1, 512, 64, 64]           1,024\n",
      "           Conv2d-45           [1, 512, 64, 64]         131,072\n",
      "      BatchNorm2d-46           [1, 512, 64, 64]           1,024\n",
      "             ReLU-47           [1, 512, 64, 64]               0\n",
      "       Bottleneck-48           [1, 512, 64, 64]               0\n",
      "           Conv2d-49           [1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-50           [1, 128, 64, 64]             256\n",
      "             ReLU-51           [1, 128, 64, 64]               0\n",
      "           Conv2d-52           [1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-53           [1, 128, 64, 64]             256\n",
      "             ReLU-54           [1, 128, 64, 64]               0\n",
      "           Conv2d-55           [1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-56           [1, 512, 64, 64]           1,024\n",
      "             ReLU-57           [1, 512, 64, 64]               0\n",
      "       Bottleneck-58           [1, 512, 64, 64]               0\n",
      "           Conv2d-59           [1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-60           [1, 128, 64, 64]             256\n",
      "             ReLU-61           [1, 128, 64, 64]               0\n",
      "           Conv2d-62           [1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-63           [1, 128, 64, 64]             256\n",
      "             ReLU-64           [1, 128, 64, 64]               0\n",
      "           Conv2d-65           [1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-66           [1, 512, 64, 64]           1,024\n",
      "             ReLU-67           [1, 512, 64, 64]               0\n",
      "       Bottleneck-68           [1, 512, 64, 64]               0\n",
      "           Conv2d-69           [1, 128, 64, 64]          65,536\n",
      "      BatchNorm2d-70           [1, 128, 64, 64]             256\n",
      "             ReLU-71           [1, 128, 64, 64]               0\n",
      "           Conv2d-72           [1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-73           [1, 128, 64, 64]             256\n",
      "             ReLU-74           [1, 128, 64, 64]               0\n",
      "           Conv2d-75           [1, 512, 64, 64]          65,536\n",
      "      BatchNorm2d-76           [1, 512, 64, 64]           1,024\n",
      "             ReLU-77           [1, 512, 64, 64]               0\n",
      "       Bottleneck-78           [1, 512, 64, 64]               0\n",
      "           Conv2d-79           [1, 256, 64, 64]         131,072\n",
      "      BatchNorm2d-80           [1, 256, 64, 64]             512\n",
      "             ReLU-81           [1, 256, 64, 64]               0\n",
      "           Conv2d-82           [1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-83           [1, 256, 32, 32]             512\n",
      "             ReLU-84           [1, 256, 32, 32]               0\n",
      "           Conv2d-85          [1, 1024, 32, 32]         262,144\n",
      "      BatchNorm2d-86          [1, 1024, 32, 32]           2,048\n",
      "           Conv2d-87          [1, 1024, 32, 32]         524,288\n",
      "      BatchNorm2d-88          [1, 1024, 32, 32]           2,048\n",
      "             ReLU-89          [1, 1024, 32, 32]               0\n",
      "       Bottleneck-90          [1, 1024, 32, 32]               0\n",
      "           Conv2d-91           [1, 256, 32, 32]         262,144\n",
      "      BatchNorm2d-92           [1, 256, 32, 32]             512\n",
      "             ReLU-93           [1, 256, 32, 32]               0\n",
      "           Conv2d-94           [1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-95           [1, 256, 32, 32]             512\n",
      "             ReLU-96           [1, 256, 32, 32]               0\n",
      "           Conv2d-97          [1, 1024, 32, 32]         262,144\n",
      "      BatchNorm2d-98          [1, 1024, 32, 32]           2,048\n",
      "             ReLU-99          [1, 1024, 32, 32]               0\n",
      "      Bottleneck-100          [1, 1024, 32, 32]               0\n",
      "          Conv2d-101           [1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-102           [1, 256, 32, 32]             512\n",
      "            ReLU-103           [1, 256, 32, 32]               0\n",
      "          Conv2d-104           [1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-105           [1, 256, 32, 32]             512\n",
      "            ReLU-106           [1, 256, 32, 32]               0\n",
      "          Conv2d-107          [1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-108          [1, 1024, 32, 32]           2,048\n",
      "            ReLU-109          [1, 1024, 32, 32]               0\n",
      "      Bottleneck-110          [1, 1024, 32, 32]               0\n",
      "          Conv2d-111           [1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-112           [1, 256, 32, 32]             512\n",
      "            ReLU-113           [1, 256, 32, 32]               0\n",
      "          Conv2d-114           [1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-115           [1, 256, 32, 32]             512\n",
      "            ReLU-116           [1, 256, 32, 32]               0\n",
      "          Conv2d-117          [1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-118          [1, 1024, 32, 32]           2,048\n",
      "            ReLU-119          [1, 1024, 32, 32]               0\n",
      "      Bottleneck-120          [1, 1024, 32, 32]               0\n",
      "          Conv2d-121           [1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-122           [1, 256, 32, 32]             512\n",
      "            ReLU-123           [1, 256, 32, 32]               0\n",
      "          Conv2d-124           [1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-125           [1, 256, 32, 32]             512\n",
      "            ReLU-126           [1, 256, 32, 32]               0\n",
      "          Conv2d-127          [1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-128          [1, 1024, 32, 32]           2,048\n",
      "            ReLU-129          [1, 1024, 32, 32]               0\n",
      "      Bottleneck-130          [1, 1024, 32, 32]               0\n",
      "          Conv2d-131           [1, 256, 32, 32]         262,144\n",
      "     BatchNorm2d-132           [1, 256, 32, 32]             512\n",
      "            ReLU-133           [1, 256, 32, 32]               0\n",
      "          Conv2d-134           [1, 256, 32, 32]         589,824\n",
      "     BatchNorm2d-135           [1, 256, 32, 32]             512\n",
      "            ReLU-136           [1, 256, 32, 32]               0\n",
      "          Conv2d-137          [1, 1024, 32, 32]         262,144\n",
      "     BatchNorm2d-138          [1, 1024, 32, 32]           2,048\n",
      "            ReLU-139          [1, 1024, 32, 32]               0\n",
      "      Bottleneck-140          [1, 1024, 32, 32]               0\n",
      "          Conv2d-141           [1, 512, 32, 32]         524,288\n",
      "     BatchNorm2d-142           [1, 512, 32, 32]           1,024\n",
      "            ReLU-143           [1, 512, 32, 32]               0\n",
      "          Conv2d-144           [1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-145           [1, 512, 16, 16]           1,024\n",
      "            ReLU-146           [1, 512, 16, 16]               0\n",
      "          Conv2d-147          [1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-148          [1, 2048, 16, 16]           4,096\n",
      "          Conv2d-149          [1, 2048, 16, 16]       2,097,152\n",
      "     BatchNorm2d-150          [1, 2048, 16, 16]           4,096\n",
      "            ReLU-151          [1, 2048, 16, 16]               0\n",
      "      Bottleneck-152          [1, 2048, 16, 16]               0\n",
      "          Conv2d-153           [1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-154           [1, 512, 16, 16]           1,024\n",
      "            ReLU-155           [1, 512, 16, 16]               0\n",
      "          Conv2d-156           [1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-157           [1, 512, 16, 16]           1,024\n",
      "            ReLU-158           [1, 512, 16, 16]               0\n",
      "          Conv2d-159          [1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-160          [1, 2048, 16, 16]           4,096\n",
      "            ReLU-161          [1, 2048, 16, 16]               0\n",
      "      Bottleneck-162          [1, 2048, 16, 16]               0\n",
      "          Conv2d-163           [1, 512, 16, 16]       1,048,576\n",
      "     BatchNorm2d-164           [1, 512, 16, 16]           1,024\n",
      "            ReLU-165           [1, 512, 16, 16]               0\n",
      "          Conv2d-166           [1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-167           [1, 512, 16, 16]           1,024\n",
      "            ReLU-168           [1, 512, 16, 16]               0\n",
      "          Conv2d-169          [1, 2048, 16, 16]       1,048,576\n",
      "     BatchNorm2d-170          [1, 2048, 16, 16]           4,096\n",
      "            ReLU-171          [1, 2048, 16, 16]               0\n",
      "      Bottleneck-172          [1, 2048, 16, 16]               0\n",
      "AdaptiveAvgPool2d-173            [1, 2048, 1, 1]               0\n",
      "          Linear-174                     [1, 2]           4,098\n",
      "          ResNet-175                     [1, 2]               0\n",
      "================================================================\n",
      "Total params: 23,505,858\n",
      "Trainable params: 23,505,858\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 1497.02\n",
      "Params size (MB): 89.67\n",
      "Estimated Total Size (MB): 1587.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=LiverResnet()\n",
    "torchsummary.summary(model.cuda(), input_size=(1,512,512),batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([transforms.ToTensor()\n",
    "                                       ])\n",
    "transforms_test = transforms.Compose([transforms.ToTensor()\n",
    "                                       ])\n",
    "\n",
    "hyper_param_epoch=10\n",
    "hyper_param_batch=8\n",
    "hyper_param_learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vol_train, vol_valid, lab_train, lab_valid = train_test_split(volume_list, all_labels, test_size=0.3, shuffle=True, stratify=all_labels, random_state=34)\n",
    "train_dataset=CustomDataset(volume_list=vol_list_train, all_labels=labels_train,transforms=transforms_train)\n",
    "test_dataset=CustomDataset(volume_list=vol_list_test,all_labels=labels_test,transforms=transforms_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyper_param_batch, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyper_param_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 / 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device,\"/\" ,torch.cuda.device_count())\n",
    "\n",
    "custom_model=LiverResnet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(custom_model.parameters(), lr=hyper_param_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(model):\n",
    "    total_loss=0\n",
    "    for i_batch, item in enumerate(test_loader):\n",
    "        npys = item['npy'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "        if(len(labels)!=hyper_param_batch):\n",
    "            break\n",
    "        outputs =model(npys)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "    return total_loss/(i_batch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],i_batch=2414 ,Train_Loss: 0.8110,Valid_loss: 0.6006\n",
      "Time: 2468.0782556533813sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-06ec313b1a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_value=1\n",
    "start=time.time()\n",
    "custom_model.train()\n",
    "train_loss_history=[]\n",
    "valid_loss_history=[]\n",
    "for e in range(hyper_param_epoch):\n",
    "        for i_batch, item in enumerate(train_loader):\n",
    "                npys = item['npy'].to(device)\n",
    "                labels = item['label'].to(device)\n",
    "                if(len(labels)!=hyper_param_batch):\n",
    "                        break\n",
    "                #print(npys)\n",
    "                # Forward pass\n",
    "                outputs =custom_model(npys)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        val_loss=validation_loss(custom_model)\n",
    "        train_loss_history.append(loss.item())\n",
    "        valid_loss_history.append(val_loss)\n",
    "        print('Epoch [{}/{}],i_batch={} ,Train_Loss: {:.4f},Valid_loss: {:.4f}'\n",
    "                                        .format(e + 1, hyper_param_epoch, i_batch+1, loss.item(),val_loss))\n",
    "        print(\"Time: {}sec\".format(time.time()-start))\n",
    "        start=time.time()\n",
    "        if loss_value>val_loss:\n",
    "                loss_value=val_loss\n",
    "                torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': custom_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, '/home/sumins/workspace/model_check/resnet50.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.plot(valid_loss_history)\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open(\"valid_loss_history50.csv\", 'w') as file:\n",
    "  writer = csv.writer(file)\n",
    "  writer.writerow(valid_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open(\"train_loss_history50.csv\", 'w') as file:\n",
    "  writer = csv.writer(file)\n",
    "  writer.writerow(train_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor(0.1801, device='cuda:0', requires_grad=True)\n",
      "Test Accuracy of the model on the 2897 test images: 82.87884017949602 %\n"
     ]
    }
   ],
   "source": [
    "true_label=[]\n",
    "pred_label=[]\n",
    "\n",
    "model=LiverResnet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_param_learning_rate)\n",
    "\n",
    "checkpoint = torch.load('/home/sumins/workspace/model_check/model_resnet18.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(epoch)\n",
    "print(loss)\n",
    "custom_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for item in test_loader:\n",
    "        npys = item['npy'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "        \n",
    "        outputs =model(npys)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        true_label.extend(labels)\n",
    "        pred_label.extend(predicted)\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the {} test images: {} %'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      2155\n",
      "           1       0.66      0.68      0.67       742\n",
      "\n",
      "    accuracy                           0.83      2897\n",
      "   macro avg       0.77      0.78      0.78      2897\n",
      "weighted avg       0.83      0.83      0.83      2897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels=torch.tensor(true_label)\n",
    "true_labels=true_labels.tolist()\n",
    "pred_labels=torch.tensor(pred_label)\n",
    "pred_labels=pred_labels.tolist()\n",
    "print(classification_report(true_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1801, device='cuda:0', requires_grad=True)\n",
      "Test Accuracy of the model on the 19310 train images: 95.90367685137235 %\n"
     ]
    }
   ],
   "source": [
    "true_label1=[]\n",
    "pred_label1=[]\n",
    "\n",
    "model=LiverResnet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_param_learning_rate)\n",
    "\n",
    "checkpoint = torch.load('/home/sumins/workspace/model_check/model_resnet18.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "print(loss)\n",
    "custom_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for item in train_loader:\n",
    "        npys = item['npy'].to(device)\n",
    "        labels = item['label'].to(device)\n",
    "        \n",
    "        outputs =model(npys)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        true_label1.extend(labels)\n",
    "        pred_label1.extend(predicted)\n",
    "        total += len(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the {} train images: {} %'.format(total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97     13062\n",
      "           1       0.94      0.94      0.94      6248\n",
      "\n",
      "    accuracy                           0.96     19310\n",
      "   macro avg       0.95      0.95      0.95     19310\n",
      "weighted avg       0.96      0.96      0.96     19310\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels1=torch.tensor(true_label1)\n",
    "true_labels1=true_labels1.tolist()\n",
    "pred_labels1=torch.tensor(pred_label1)\n",
    "pred_labels1=pred_labels1.tolist()\n",
    "print(classification_report(true_labels1,pred_labels1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
